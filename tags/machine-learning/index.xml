<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine Learning on mike.dev</title><link>https://mike.dev/tags/machine-learning/</link><description>Recent content in Machine Learning on mike.dev</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 07 Oct 2024 13:11:00 -0700</lastBuildDate><atom:link href="https://mike.dev/tags/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Learning about Distributed Inference with DeepSpeed ZeRO-3 and Docker Compose</title><link>https://mike.dev/blog/distributed-inference-deepspeed/</link><pubDate>Mon, 07 Oct 2024 13:11:00 -0700</pubDate><guid>https://mike.dev/blog/distributed-inference-deepspeed/</guid><description>&lt;p>Today, we&amp;rsquo;re going to test out DeepSpeed ZeRO-3 in docker-compose. Perhaps in a future blog post, I&amp;rsquo;ll cover &lt;code>DeepSpeed-FastGen&lt;/code> or how to deploy this on a real multi-node/multi-gpu cluster. I also aim to compare this method vs Multi-Node Inference with &lt;code>vLLM&lt;/code>. If you&amp;rsquo;re setting up a local cluster, consider checking out high bandwidth networking with InfiniBand. It&amp;rsquo;s surprisingly affordable.&lt;/p></description></item></channel></rss>