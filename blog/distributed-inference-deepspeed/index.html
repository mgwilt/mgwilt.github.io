<!doctype html><html itemscope class=dark lang=en-us itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=5"><meta name=theme-name content="hugoplate"><link rel="shortcut icon" href=/images/favicon_hu9880217891310920762.png type=image/x-icon><link rel=icon href=/images/favicon_hu9880217891310920762.png type=image/x-icon><link rel=icon type=image/png sizes=48x48 href=/images/favicon_hu11285212527156944795.png><link rel=icon type=image/png sizes=96x96 href=/images/favicon_hu9880217891310920762.png><link rel=apple-touch-icon sizes=144x144 href=/images/favicon_hu290619398646655712.png><link rel=manifest href=/manifest.webmanifest><meta name=msapplication-TileColor content="#ddd"><meta name=theme-color content="#ffffff"><base href=https://mike.dev/blog/distributed-inference-deepspeed/><title>Learning about Distributed Inference with DeepSpeed ZeRO-3 and Docker Compose</title>
<meta name=keywords content="AWS,CSharp,Dotnet,Python,DevOps"><meta name=description content="Learn how to set up distributed inference using DeepSpeed ZeRO-3 and Docker Compose."><meta name=author content="Michael Gwilt"><meta property="og:image" content="https://mike.dev/images/og-image.png"><meta name=twitter:image content="https://mike.dev/images/og-image.png"><meta name=twitter:card content="summary_large_image"><meta property="og:image:width" content="900"><meta property="og:image:height" content="600"><meta property="og:image:type" content="image/.png"><meta property="og:title" content="Learning about Distributed Inference with DeepSpeed ZeRO-3 and Docker Compose"><meta property="og:description" content="Learn how to set up distributed inference using DeepSpeed ZeRO-3 and Docker Compose."><meta property="og:type" content="website"><meta property="og:url" content="https://mike.dev/blog/distributed-inference-deepspeed/"><meta name=twitter:title content="Learning about Distributed Inference with DeepSpeed ZeRO-3 and Docker Compose"><meta name=twitter:description content="Learn how to set up distributed inference using DeepSpeed ZeRO-3 and Docker Compose."><meta name=twitter:site content="@mgwilt"><meta name=twitter:creator content="@Michael Gwilt"><script>let indexURL="https://mike.dev/searchindex.json",includeSectionsInSearch=["blog"],search_no_results="No results for",search_initial_message="Type something to search.."</script><meta http-equiv=x-dns-prefetch-control content="on"><link rel=preconnect href=https://use.fontawesome.com crossorigin><link rel=preconnect href=//cdnjs.cloudflare.com><link rel=preconnect href=//www.googletagmanager.com><link rel=preconnect href=//www.google-analytics.com><link rel=dns-prefetch href=https://use.fontawesome.com><link rel=dns-prefetch href=//ajax.googleapis.com><link rel=dns-prefetch href=//cdnjs.cloudflare.com><link rel=dns-prefetch href=//www.googletagmanager.com><link rel=dns-prefetch href=//www.google-analytics.com><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//connect.facebook.net><link rel=dns-prefetch href=//platform.linkedin.com><link rel=dns-prefetch href=//platform.twitter.com><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Heebo:wght@400;600&family=Signika:wght@500;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script><link href="/css/style.min.cef59e05d78e7af64bae037a5daadb8acb6556ecbc966d961c5283fab22f46f5.css" integrity="sha256-zvWeBdeOevZLrgN6XarbistlVuy8lm2WHFKD+rIvRvU=" rel=stylesheet><link defer async rel=stylesheet href="/css/style-lazy.min.5d85e889fd5f5b28f9387aeade847fcb0a66d76a99a985ab4cdae3d37cf21557.css" integrity="sha256-XYXoif1fWyj5OHrq3oR/ywpm12qZqYWrTNrj03zyFVc=" media=print onload='this.media="all",this.onload=null'></head><body><header class="header sticky top-0 z-30"><nav class="navbar container"><div class=order-0><a class="navbar-brand block" href=/></a></div><input id=nav-toggle type=checkbox class=hidden>
<label for=nav-toggle class="order-3 cursor-pointer flex items-center lg:hidden text-dark dark:text-white lg:order-1"><svg id="show-button" class="h-6 fill-current block" viewBox="0 0 20 20"><title>Menu Open</title><path d="M0 3h20v2H0V3zm0 6h20v2H0V9zm0 6h20v2H0V0z"/></svg><svg id="hide-button" class="h-6 fill-current hidden" viewBox="0 0 20 20"><title>Menu Close</title><polygon points="11 9 22 9 22 11 11 11 11 22 9 22 9 11 -2 11 -2 9 9 9 9 -2 11 -2" transform="rotate(45 10 10)"/></svg></label><ul id=nav-menu class="navbar-nav order-3 hidden lg:flex w-full pb-6 lg:order-1 lg:w-auto lg:space-x-2 lg:pb-0 xl:space-x-8"><li class=nav-item><a class=nav-link href=/>Home</a></li><li class=nav-item><a class=nav-link href=/blog>Blog</a></li></ul><div class="order-1 ml-auto flex items-center md:order-2 lg:ml-0"><button aria-label=search class="border-border text-dark hover:text-primary dark:border-darkmode-border mr-5 inline-block border-r lg:border-r-0 pr-5 lg:pr-0 text-xl dark:text-white dark:hover:text-darkmode-primary" data-target=search-modal>
<i class="fa-solid fa-search"></i></button></div></nav></header><div class=search-modal aria-hidden=true style=--color-primary:#121212><div data-target=close-search-modal class=search-modal-overlay></div><div class=search-wrapper data-image=true data-description=true data-tags=true data-categories=true><div class=search-wrapper-header><label for=search-modal-input style=margin-top:-1px><span class=sr-only>search icon</span>
<svg viewBox="0 0 512 512" height="18" width="18" class="search-icon" data-type="search"><path fill="currentcolor" d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8.0 45.3s-32.8 12.5-45.3.0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9.0 208S93.1.0 208 0 416 93.1 416 208zM208 352a144 144 0 100-288 144 144 0 100 288z"/></svg>
<svg viewBox="0 0 512 512" height="18" width="18" class="search-reset" data-type="reset"><path fill="currentcolor" d="M256 512A256 256 0 10256 0a256 256 0 100 512zM175 175c9.4-9.4 24.6-9.4 33.9.0l47 47 47-47c9.4-9.4 24.6-9.4 33.9.0s9.4 24.6.0 33.9l-47 47 47 47c9.4 9.4 9.4 24.6.0 33.9s-24.6 9.4-33.9.0l-47-47-47 47c-9.4 9.4-24.6 9.4-33.9.0s-9.4-24.6.0-33.9l47-47-47-47c-9.4-9.4-9.4-24.6.0-33.9z"/></svg>
</label><input id=search-modal-input type=text data-search-input autocomplete=off aria-label=Search placeholder="Search Post..."></div><div class=search-wrapper-body><div class=search-result data-search-result></div><span class=search-result-empty>Type something to search..</span></div><div class=search-wrapper-footer><span><kbd><svg width="14" height="14" fill="currentcolor" viewBox="0 0 16 16"><path d="M3.204 11h9.592L8 5.519 3.204 11zm-.753-.659 4.796-5.48a1 1 0 011.506.0l4.796 5.48c.566.647.106 1.659-.753 1.659H3.204a1 1 0 01-.753-1.659z"/></svg>
</kbd><kbd><svg width="14" height="14" fill="currentcolor" style="margin-top:1px" viewBox="0 0 16 16"><path d="M3.204 5h9.592L8 10.481 3.204 5zm-.753.659 4.796 5.48a1 1 0 001.506.0l4.796-5.48c.566-.647.106-1.659-.753-1.659H3.204a1 1 0 00-.753 1.659z"/></svg>
</kbd>to navigate
</span><span><kbd><svg width="12" height="12" fill="currentcolor" style="display:inline-block" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M14.5 1.5a.5.5.0 01.5.5v4.8a2.5 2.5.0 01-2.5 2.5H2.707l3.347 3.346a.5.5.0 01-.708.708l-4.2-4.2a.5.5.0 010-.708l4-4a.5.5.0 11.708.708L2.707 8.3H12.5A1.5 1.5.0 0014 6.8V2a.5.5.0 01.5-.5z"/></svg>
</kbd>to select
</span><span class=search-result-info></span>
<span data-target=close-search-modal><kbd>ESC</kbd> to close</span></div></div></div><main><section class="section pt-7"><div class=container><div class="row justify-center"><article class=lg:col-10><h1 class="h2 mb-4">Learning about Distributed Inference with DeepSpeed ZeRO-3 and Docker Compose</h1><ul class=mb-4><li class="mr-4 inline-block"><a href=/authors/michael-gwilt/><i class="fa-regular fa-circle-user mr-2"></i>Michael Gwilt</a></li><li class="mr-4 inline-block"><i class="fa-regular fa-folder mr-2"></i>
<a href=/categories/machine-learning/>Machine learning
,
</a><a href=/categories/devops/>Dev ops</a></li><li class="mr-4 inline-block"><i class="fa-regular fa-clock mr-2"></i>
October 7, 2024</li></ul><div class="content mb-10"><p>Today, we&rsquo;re going to test out DeepSpeed ZeRO-3 in docker-compose. Perhaps in a future blog post, I&rsquo;ll cover <code>DeepSpeed-FastGen</code> or how to deploy this on a real multi-node/multi-gpu cluster. I also aim to compare this method vs Multi-Node Inference with <code>vLLM</code>. If you&rsquo;re setting up a local cluster, consider checking out high bandwidth networking with InfiniBand. It&rsquo;s surprisingly affordable.</p><h2 id=what-is-deepspeed>What is DeepSpeed?</h2><p>DeepSpeed ZeRO-3 (Zero Redundancy Optimizer) is an optimization technique developed by Microsoft that enables efficient large-scale model training and inference. It&rsquo;s particularly useful for distributing large language models across multiple GPUs or nodes. Here&rsquo;s a brief overview of its key features:</p><ol><li><p>Memory Efficiency: ZeRO-3 partitions model parameters, gradients, and optimizer states across GPUs, significantly reducing memory requirements per device.</p></li><li><p>Computation Efficiency: It eliminates redundant computations in data-parallel training, improving overall efficiency.</p></li><li><p>Scalability: ZeRO-3 allows for training and inference of models that are much larger than what can fit on a single GPU.</p></li><li><p>Communication Optimization: It implements efficient communication algorithms to minimize data transfer between devices.</p></li><li><p>Easy Integration: DeepSpeed can be integrated with existing PyTorch models with minimal code changes.</p></li></ol><p>While ZeRO-3 shines in multi-GPU setups, even on a single GPU it can provide benefits in terms of memory management, potentially allowing you to work with larger models or batch sizes than would otherwise be possible. In our single-GPU setup, we won&rsquo;t see performance improvements, but we&rsquo;ll gain valuable experience in configuring and deploying a distributed inference system.</p><h2 id=project-structure>Project Structure</h2><pre tabindex=0><code>FancyInference/
├── deepspeed_config.json
├── docker-compose.yml
├── Dockerfile
├── entrypoint.sh
├── hostfile.txt
├── inference.py
└── requirements.txt
</code></pre><h2 id=configuring-our-hosts>Configuring our Hosts</h2><p>Here&rsquo;s our dockerfile. We use <code>entrypoint.sh</code> because I&rsquo;d like to use the deepspeed launcher and I think it&rsquo;s cleaner to have this in its own file instead of a long entrypoint string.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-docker data-lang=docker><span style=display:flex><span><span style=color:#66d9ef>FROM</span><span style=color:#e6db74> pytorch/pytorch:1.13.1-cuda11.6-cudnn8-runtime</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>RUN</span> apt-get update <span style=color:#f92672>&amp;&amp;</span> apt-get install -y <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    git <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    wget <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    curl <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    <span style=color:#f92672>&amp;&amp;</span> rm -rf /var/lib/apt/lists/*<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>ENV</span> PYTHONUNBUFFERED<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>ENV</span> DEBIAN_FRONTEND<span style=color:#f92672>=</span>noninteractive
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>RUN</span> pip install --upgrade pip<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>RUN</span> pip install deepspeed transformers<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>WORKDIR</span><span style=color:#e6db74> /app</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>COPY</span> inference.py .<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>COPY</span> deepspeed_config.json .<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>COPY</span> hostfile.txt .<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>COPY</span> entrypoint.sh .<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>RUN</span> chmod +x entrypoint.sh<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>ENTRYPOINT</span> [<span style=color:#e6db74>&#34;/app/entrypoint.sh&#34;</span>]<span style=color:#960050;background-color:#1e0010>
</span></span></span></code></pre></div><p>Our <code>docker-compose.yml</code> will set up 3 hosts.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>version</span>: <span style=color:#e6db74>&#39;3.8&#39;</span>  <span style=color:#75715e># Ensure you specify the version</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>services</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>worker-1</span>: <span style=color:#75715e># TAKE NOTE OF THE NAME HERE, AS THIS WILL BE OUR HOSTNAME IN HOSTS.TXT</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>build</span>: <span style=color:#ae81ff>.</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>container_name</span>: <span style=color:#ae81ff>worker-1</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>environment</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>RANK</span>: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>WORLD_SIZE</span>: <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>MASTER_ADDR</span>: <span style=color:#ae81ff>worker-1</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>MASTER_PORT</span>: <span style=color:#ae81ff>12345</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>NVIDIA_VISIBLE_DEVICES</span>: <span style=color:#ae81ff>0</span> <span style=color:#75715e># this is device 0, my only GPU. It should be comma separated device IDs</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>networks</span>:
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>distributed-net</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>deploy</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>reservations</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>devices</span>:
</span></span><span style=display:flex><span>            - <span style=color:#f92672>capabilities</span>: [<span style=color:#ae81ff>gpu]</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>runtime</span>: <span style=color:#ae81ff>nvidia</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>worker-2</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>build</span>: <span style=color:#ae81ff>.</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>container_name</span>: <span style=color:#ae81ff>worker-2</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>environment</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>RANK</span>: <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>WORLD_SIZE</span>: <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>MASTER_ADDR</span>: <span style=color:#ae81ff>worker-1</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>MASTER_PORT</span>: <span style=color:#ae81ff>12345</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>NVIDIA_VISIBLE_DEVICES</span>: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>networks</span>:
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>distributed-net</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>deploy</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>reservations</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>devices</span>:
</span></span><span style=display:flex><span>            - <span style=color:#f92672>capabilities</span>: [<span style=color:#ae81ff>gpu]</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>runtime</span>: <span style=color:#ae81ff>nvidia</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>worker-3</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>build</span>: <span style=color:#ae81ff>.</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>container_name</span>: <span style=color:#ae81ff>worker-3</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>environment</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>RANK</span>: <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>WORLD_SIZE</span>: <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>MASTER_ADDR</span>: <span style=color:#ae81ff>worker-1</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>MASTER_PORT</span>: <span style=color:#ae81ff>12345</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>NVIDIA_VISIBLE_DEVICES</span>: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>networks</span>:
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>distributed-net</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>deploy</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>reservations</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>devices</span>:
</span></span><span style=display:flex><span>            - <span style=color:#f92672>capabilities</span>: [<span style=color:#ae81ff>gpu]</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>runtime</span>: <span style=color:#ae81ff>nvidia</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>networks</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>distributed-net</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>driver</span>: <span style=color:#ae81ff>bridge</span>
</span></span></code></pre></div><p>Here&rsquo;s <code>entrypoint.sh</code>. We&rsquo;ll let our test rely on docker-compose&rsquo;s internal DNS.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>export RANK<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>RANK<span style=color:#66d9ef>:-</span>0<span style=color:#e6db74>}</span>
</span></span><span style=display:flex><span>export WORLD_SIZE<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>WORLD_SIZE<span style=color:#66d9ef>:-</span>3<span style=color:#e6db74>}</span>
</span></span><span style=display:flex><span>export MASTER_ADDR<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>MASTER_ADDR<span style=color:#66d9ef>:-</span>worker-1<span style=color:#e6db74>}</span>
</span></span><span style=display:flex><span>export MASTER_PORT<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>MASTER_PORT<span style=color:#66d9ef>:-</span>12345<span style=color:#e6db74>}</span>
</span></span><span style=display:flex><span>export NVIDIA_VISIBLE_DEVICES<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>NVIDIA_VISIBLE_DEVICES<span style=color:#66d9ef>:-</span>0<span style=color:#e6db74>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>deepspeed --hostfile<span style=color:#f92672>=</span>hostfile.txt <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --no_ssh <span style=color:#ae81ff>\ </span><span style=color:#75715e># no_ssh is here so that our nodes can talk to each other easily.</span>
</span></span><span style=display:flex><span>    --num_gpus<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span> <span style=color:#ae81ff>\ </span><span style=color:#75715e># my machine only has 1 GPU. This should be the number of GPUs per node</span>
</span></span><span style=display:flex><span>    --num_nodes<span style=color:#f92672>=</span>$WORLD_SIZE <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --node_rank<span style=color:#f92672>=</span>$RANK <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --master_addr<span style=color:#f92672>=</span>$MASTER_ADDR <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --master_port<span style=color:#f92672>=</span>$MASTER_PORT <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    inference.py
</span></span></code></pre></div><p>Next, let&rsquo;s set up our <code>hostfile.txt</code>. This file defines the hosts that will participate in our distributed inference setup. Here&rsquo;s how we configure it:</p><ol><li>We&rsquo;re setting up 3 workers, each with 1 GPU.</li><li>The format is <code>hostname slots=number_of_GPUs</code>.</li><li>In our case, we use <code>worker-1</code>, <code>worker-2</code>, and <code>worker-3</code> as hostnames. These correspond to the container names in our <code>docker-compose</code> file.</li><li>Docker conveniently uses these container names as hostnames, making our setup easier.</li><li>If you have more GPUs per worker, you can adjust the <code>slots</code> value. For example, if <code>worker-1</code> had 2 GPUs, you&rsquo;d write <code>worker-1 slots=2</code>.</li></ol><p>Here&rsquo;s what our <code>hostfile.txt</code> looks like:</p><pre tabindex=0><code>worker-1 slots=1
worker-2 slots=1
worker-3 slots=1
</code></pre><p>Lastly, we want to define our configuration file for DeepSpeed in <code>deepspeed_config.json</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;train_batch_size&#34;</span>: <span style=color:#ae81ff>3</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;train_micro_batch_size_per_gpu&#34;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;gradient_accumulation_steps&#34;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;fp16&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;enabled&#34;</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>  },
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;zero_optimization&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;stage&#34;</span>: <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=inference>Inference</h2><p>First, define our package requirements in <code>requirements.txt</code></p><pre tabindex=0><code>torch
transformers
deepspeed
</code></pre><p>In <code>inference.py</code> we use a small sentiment model and pass in a few questions to test with.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> deepspeed
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoModelForSequenceClassification, AutoTokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>main</span>():
</span></span><span style=display:flex><span>    deepspeed<span style=color:#f92672>.</span>init_distributed()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    rank <span style=color:#f92672>=</span> int(os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;RANK&#34;</span>, <span style=color:#ae81ff>0</span>))
</span></span><span style=display:flex><span>    world_size <span style=color:#f92672>=</span> int(os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;WORLD_SIZE&#34;</span>, <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    model_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;distilbert-base-uncased-finetuned-sst-2-english&#34;</span>
</span></span><span style=display:flex><span>    tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_name)
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> AutoModelForSequenceClassification<span style=color:#f92672>.</span>from_pretrained(model_name)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    ds_config <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;deepspeed_config.json&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    model_engine, _, _, _ <span style=color:#f92672>=</span> deepspeed<span style=color:#f92672>.</span>initialize(
</span></span><span style=display:flex><span>        model<span style=color:#f92672>=</span>model,
</span></span><span style=display:flex><span>        config<span style=color:#f92672>=</span>ds_config,
</span></span><span style=display:flex><span>        model_parameters<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    questions <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;I absolutely love this movie, it&#39;s a masterpiece!&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;The service at this restaurant was terrible and the food was bland.&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;I&#39;m feeling quite neutral about the whole situation.&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;This new gadget is amazing, it has exceeded all my expectations!&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;I&#39;m really disappointed with the outcome of the game.&#34;</span>
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    labels <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;Negative&#34;</span>, <span style=color:#e6db74>&#34;Positive&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> input_text <span style=color:#f92672>in</span> questions:
</span></span><span style=display:flex><span>        inputs <span style=color:#f92672>=</span> tokenizer(input_text, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>)
</span></span><span style=display:flex><span>        inputs <span style=color:#f92672>=</span> {key: value<span style=color:#f92672>.</span>to(model_engine<span style=color:#f92672>.</span>device) <span style=color:#66d9ef>for</span> key, value <span style=color:#f92672>in</span> inputs<span style=color:#f92672>.</span>items()}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>            outputs <span style=color:#f92672>=</span> model_engine(<span style=color:#f92672>**</span>inputs)
</span></span><span style=display:flex><span>            logits <span style=color:#f92672>=</span> outputs<span style=color:#f92672>.</span>logits
</span></span><span style=display:flex><span>            predictions <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>argmax(logits, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> rank <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Input: </span><span style=color:#e6db74>{</span>input_text<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Prediction: </span><span style=color:#e6db74>{</span>labels[predictions<span style=color:#f92672>.</span>item()]<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Worker </span><span style=color:#e6db74>{</span>rank<span style=color:#e6db74>}</span><span style=color:#e6db74>: Inference completed for input: </span><span style=color:#e6db74>{</span>input_text<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>        main()
</span></span></code></pre></div><p>To Test, navigate to your project folder and run:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker compose build
</span></span><span style=display:flex><span>docker compose up
</span></span></code></pre></div><p>After all of your workers initialize and the model is downloaded you should see each worker process our input questions and we expect Worker-1 to deliver the result for each.</p></div><div class="row items-start justify-between"><div class="lg:col-5 mb-10 flex items-center lg:mb-0"><h5 class=mr-3>Tags :</h5><ul><li class=inline-block><a class="bg-theme-light hover:bg-primary dark:bg-darkmode-theme-light dark:hover:bg-darkmode-primary dark:hover:text-dark m-1 block rounded px-3 py-1 hover:text-white" href=/tags/deepspeed/>Deep speed</a></li><li class=inline-block><a class="bg-theme-light hover:bg-primary dark:bg-darkmode-theme-light dark:hover:bg-darkmode-primary dark:hover:text-dark m-1 block rounded px-3 py-1 hover:text-white" href=/tags/docker/>Docker</a></li><li class=inline-block><a class="bg-theme-light hover:bg-primary dark:bg-darkmode-theme-light dark:hover:bg-darkmode-primary dark:hover:text-dark m-1 block rounded px-3 py-1 hover:text-white" href=/tags/distributed-inference/>Distributed inference</a></li><li class=inline-block><a class="bg-theme-light hover:bg-primary dark:bg-darkmode-theme-light dark:hover:bg-darkmode-primary dark:hover:text-dark m-1 block rounded px-3 py-1 hover:text-white" href=/tags/machine-learning/>Machine learning</a></li><li class=inline-block><a class="bg-theme-light hover:bg-primary dark:bg-darkmode-theme-light dark:hover:bg-darkmode-primary dark:hover:text-dark m-1 block rounded px-3 py-1 hover:text-white" href=/tags/devops/>Dev ops</a></li></ul></div><div class="lg:col-4 flex items-center"><div class=share-icons><h5 class=share-title>Share :</h5><a class="share-link share-facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmike.dev%2fblog%2fdistributed-inference-deepspeed%2f" target=_blank rel=noopener aria-label="share facebook"><span class=share-icon><svg viewBox="0 0 24 24"><path d="M18.77 7.46H14.5v-1.9c0-.9.6-1.1 1-1.1h3V.5h-4.33C10.24.5 9.5 3.44 9.5 5.32v2.15h-3v4h3v12h5v-12h3.85l.42-4z"/></svg>
</span></a><a class="share-link share-twitter" href="https://twitter.com/intent/tweet/?text=Share&amp;url=https%3a%2f%2fmike.dev%2fblog%2fdistributed-inference-deepspeed%2f" target=_blank rel=noopener aria-label="share twitter"><span aria-hidden=true class=share-icon><svg viewBox="0 0 24 24"><path d="M8 2H1l8.26 11.015L1.45 22H4.1l6.388-7.349L16 22h7l-8.608-11.478L21.8 2h-2.65l-5.986 6.886zm9 18L5 4h2l12 16z"/></svg>
</span></a><a class="share-link share-email" href="mailto:?subject=Share&amp;body=https%3a%2f%2fmike.dev%2fblog%2fdistributed-inference-deepspeed%2f" target=_self rel=noopener aria-label="share email"><span aria-hidden=true class=share-icon><svg viewBox="0 0 24 24"><path d="M22 4H2C.9 4 0 4.9.0 6v12c0 1.1.9 2 2 2h20c1.1.0 2-.9 2-2V6c0-1.1-.9-2-2-2zM7.25 14.43l-3.5 2c-.08.05-.17.07-.25.07-.17.0-.34-.1-.43-.25-.14-.24-.06-.55.18-.68l3.5-2c.24-.14.55-.06.68.18.14.24.06.55-.18.68zm4.75.07c-.1.0-.2-.03-.27-.08l-8.5-5.5c-.23-.15-.3-.46-.15-.7.15-.22.46-.3.7-.14L12 13.4l8.23-5.32c.23-.15.54-.08.7.15.14.23.07.54-.16.7l-8.5 5.5c-.08.04-.17.07-.27.07zm8.93 1.75c-.1.16-.26.25-.43.25-.08.0-.17-.02-.25-.07l-3.5-2c-.24-.13-.32-.44-.18-.68s.44-.32.68-.18l3.5 2c.24.13.32.44.18.68z"/></svg>
</span></a><a class="share-link share-reddit" href="https://reddit.com/submit/?url=https%3a%2f%2fmike.dev%2fblog%2fdistributed-inference-deepspeed%2f&amp;resubmit=true&amp;title=Share" target=_blank rel=noopener aria-label="share reddit"><span aria-hidden=true class=share-icon><svg viewBox="0 0 24 24"><path d="M24 11.5c0-1.65-1.35-3-3-3-.96.0-1.86.48-2.42 1.24-1.64-1-3.75-1.64-6.07-1.72.08-1.1.4-3.05 1.52-3.7.72-.4 1.73-.24 3 .5C17.2 6.3 18.46 7.5 20 7.5c1.65.0 3-1.35 3-3s-1.35-3-3-3c-1.38.0-2.54.94-2.88 2.22-1.43-.72-2.64-.8-3.6-.25-1.64.94-1.95 3.47-2 4.55-2.33.08-4.45.7-6.1 1.72C4.86 8.98 3.96 8.5 3 8.5c-1.65.0-3 1.35-3 3 0 1.32.84 2.44 2.05 2.84-.03.22-.05.44-.05.66.0 3.86 4.5 7 10 7s10-3.14 10-7c0-.22-.02-.44-.05-.66 1.2-.4 2.05-1.54 2.05-2.84zM2.3 13.37C1.5 13.07 1 12.35 1 11.5c0-1.1.9-2 2-2 .64.0 1.22.32 1.6.82-1.1.85-1.92 1.9-2.3 3.05zm3.7.13c0-1.1.9-2 2-2s2 .9 2 2-.9 2-2 2-2-.9-2-2zm9.8 4.8c-1.08.63-2.42.96-3.8.96-1.4.0-2.74-.34-3.8-.95-.24-.13-.32-.44-.2-.68.15-.24.46-.32.7-.18 1.83 1.06 4.76 1.06 6.6.0.23-.13.53-.05.67.2.14.23.06.54-.18.67zm.2-2.8c-1.1.0-2-.9-2-2s.9-2 2-2 2 .9 2 2-.9 2-2 2zm5.7-2.13c-.38-1.16-1.2-2.2-2.3-3.05.38-.5.97-.82 1.6-.82 1.1.0 2 .9 2 2 0 .84-.53 1.57-1.3 1.87z"/></svg></span></a></div></div></div></article></div><div class="section pb-0"><h2 class="h3 mb-12">Related Posts</h2><div class=row><div class="lg:col-4 md:col-6 mb-14"><div class="bg-body dark:bg-darkmode-body"><h4 class=mb-3><a href=/blog/aws-cdk-localstack/>How to Set Up a Serverless Home Lab with AWS CDK, Lambda, and LocalStack</a></h4><ul class=mb-4><li class="mr-4 inline-block"><a href=/authors/michael-gwilt/><i class="fa-regular fa-circle-user mr-2"></i>Michael Gwilt</a></li><li class="mr-4 inline-block"><i class="fa-regular fa-folder mr-1"></i>
<a href=/categories/cloud/ class=ms-1>Cloud
,
</a><a href=/categories/devops/ class=ms-1>Devops</a></li></ul><p class=mb-6><p>Are you looking to develop and your cloud application locally without incurring AWS costs? In this tutorial, we&rsquo;ll guide you through setting up a local serverless environment using AWS CDK, LocalStack, and a simple Python Lambda function. We&rsquo;ll leverage the PythonFunction construct from the aws-cdk.aws-lambda-python-alpha module to streamline the process.</p></p><a class="btn btn-outline-primary btn-sm" href=/blog/aws-cdk-localstack/>Read More</a></div></div></div></div></div></section></main><footer class="bg-theme-light dark:bg-darkmode-theme-light"><div class=container><div class="row items-center py-10"><div class="lg:col-3 mb-8 text-center lg:mb-0 lg:text-left"><a class="navbar-brand inline-block" href=/></a></div><div class="lg:col-6 mb-8 text-center lg:mb-0"><ul></ul></div><div class="lg:col-3 mb-8 text-center lg:mb-0 lg:mt-0 lg:text-right"><ul class=social-icons><li><a target=_blank aria-label=twitter rel="nofollow noopener" href=https://twitter.com/mgwilt><i class="fab fa-twitter"></i></a></li><li><a target=_blank aria-label=github rel="nofollow noopener" href=https://www.github.com/mgwilt><i class="fab fa-github"></i></a></li></ul></div></div></div><div class="border-border dark:border-darkmode-border border-t py-7"><div class="text-light dark:text-darkmode-light container text-center"><p></p></div></div></footer><script crossorigin=anonymous integrity="sha256-LO9JECYLqyMcpGlrtxZcpUn0I8AvhO6oIIibJZv4zbk=" src=/js/script.min.2cef4910260bab231ca4696bb7165ca549f423c02f84eea820889b259bf8cdb9.js></script><script defer async crossorigin=anonymous integrity="sha256-w+aS42D2+B+Jix+joZ7pAua1vbu/pRK/IhoP55b8n3w=" src=/js/script-lazy.min.c3e692e360f6f81f898b1fa3a19ee902e6b5bdbbbfa512bf221a0fe796fc9f7c.js></script><script>"serviceWorker"in navigator&&navigator.serviceWorker.register("/service-worker.js")</script></body></html>