[{"section":"Blog","slug":"/blog/distributed-inference-deepspeed/","title":"Learning about Distributed Inference with DeepSpeed ZeRO-3 and Docker Compose","description":"Learn how to set up distributed inference using DeepSpeed ZeRO-3 and Docker Compose.","date":"October 7, 2024","image":null,"imageSM":null,"searchKeyword":"","categories":"Machine Learning, DevOps","tags":"DeepSpeed, Docker, Distributed Inference, Machine Learning, DevOps","content":"Today, we\u0026rsquo;re going to test out DeepSpeed ZeRO-3 in docker-compose. Perhaps in a future blog post, I\u0026rsquo;ll cover DeepSpeed-FastGen or how to deploy this on a real multi-node/multi-gpu cluster. I also aim to compare this method vs Multi-Node Inference with vLLM. If you\u0026rsquo;re setting up a local cluster, consider checking out high bandwidth networking with InfiniBand. It\u0026rsquo;s surprisingly affordable.\nWhat is DeepSpeed? DeepSpeed ZeRO-3 (Zero Redundancy Optimizer) is an optimization technique developed by Microsoft that enables efficient large-scale model training and inference. It\u0026rsquo;s particularly useful for distributing large language models across multiple GPUs or nodes. Here\u0026rsquo;s a brief overview of its key features:\nMemory Efficiency: ZeRO-3 partitions model parameters, gradients, and optimizer states across GPUs, significantly reducing memory requirements per device.\nComputation Efficiency: It eliminates redundant computations in data-parallel training, improving overall efficiency.\nScalability: ZeRO-3 allows for training and inference of models that are much larger than what can fit on a single GPU.\nCommunication Optimization: It implements efficient communication algorithms to minimize data transfer between devices.\nEasy Integration: DeepSpeed can be integrated with existing PyTorch models with minimal code changes.\nWhile ZeRO-3 shines in multi-GPU setups, even on a single GPU it can provide benefits in terms of memory management, potentially allowing you to work with larger models or batch sizes than would otherwise be possible. In our single-GPU setup, we won\u0026rsquo;t see performance improvements, but we\u0026rsquo;ll gain valuable experience in configuring and deploying a distributed inference system.\nProject Structure FancyInference/\r├── deepspeed_config.json\r├── docker-compose.yml\r├── Dockerfile\r├── entrypoint.sh\r├── hostfile.txt\r├── inference.py\r└── requirements.txt Configuring our Hosts Here\u0026rsquo;s our dockerfile. We use entrypoint.sh because I\u0026rsquo;d like to use the deepspeed launcher and I think it\u0026rsquo;s cleaner to have this in its own file instead of a long entrypoint string.\nFROM pytorch/pytorch:1.13.1-cuda11.6-cudnn8-runtime RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ git \\ wget \\ curl \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* ENV PYTHONUNBUFFERED=1 ENV DEBIAN_FRONTEND=noninteractive RUN pip install --upgrade pip RUN pip install deepspeed transformers WORKDIR /app COPY inference.py . COPY deepspeed_config.json . COPY hostfile.txt . COPY entrypoint.sh . RUN chmod +x entrypoint.sh ENTRYPOINT [\u0026#34;/app/entrypoint.sh\u0026#34;] Our docker-compose.yml will set up 3 hosts.\nservices: worker-1: build: . container_name: worker-1 # TAKE NOTE OF THE NAME HERE, AS THIS WILL BE OUR HOSTNAME IN HOSTS.TXT environment: RANK: 0 WORLD_SIZE: 3 MASTER_ADDR: worker-1 MASTER_PORT: 12345 NVIDIA_VISIBLE_DEVICES: 0 # this is device 0, my only GPU. It should be comma separated device IDs networks: - distributed-net deploy: resources: reservations: devices: - capabilities: [gpu] runtime: nvidia worker-2: build: . container_name: worker-2 environment: RANK: 1 WORLD_SIZE: 3 MASTER_ADDR: worker-1 MASTER_PORT: 12345 NVIDIA_VISIBLE_DEVICES: 0 networks: - distributed-net deploy: resources: reservations: devices: - capabilities: [gpu] runtime: nvidia worker-3: build: . container_name: worker-3 environment: RANK: 2 WORLD_SIZE: 3 MASTER_ADDR: worker-1 MASTER_PORT: 12345 NVIDIA_VISIBLE_DEVICES: 0 networks: - distributed-net deploy: resources: reservations: devices: - capabilities: [gpu] runtime: nvidia networks: distributed-net: driver: bridge Here\u0026rsquo;s entrypoint.sh. We\u0026rsquo;ll let our test rely on docker-compose\u0026rsquo;s internal DNS.\n#!/bin/bash export RANK=${RANK:-0} export WORLD_SIZE=${WORLD_SIZE:-3} export MASTER_ADDR=${MASTER_ADDR:-worker-1} export MASTER_PORT=${MASTER_PORT:-12345} export NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-0} deepspeed --hostfile=hostfile.txt \\ --no_ssh \\ # no_ssh is here so that our nodes can talk to each other easily. --num_gpus=1 \\ # my machine only has 1 GPU. This should be the number of GPUs per node --num_nodes=$WORLD_SIZE \\ --node_rank=$RANK \\ --master_addr=$MASTER_ADDR \\ --master_port=$MASTER_PORT \\ inference.py Next, let\u0026rsquo;s set up our hostfile.txt. This file defines the hosts that will participate in our distributed inference setup. Here\u0026rsquo;s how we configure it:\nWe\u0026rsquo;re setting up 3 workers, each with 1 GPU. The format is hostname slots=number_of_GPUs. In our case, we use worker-1, worker-2, and worker-3 as hostnames. These correspond to the container names in our docker-compose file. Docker conveniently uses these container names as hostnames, making our setup easier. If you have more GPUs per worker, you can adjust the slots value. For example, if worker-1 had 2 GPUs, you\u0026rsquo;d write worker-1 slots=2. Here\u0026rsquo;s what our hostfile.txt looks like:\nworker-1 slots=1\rworker-2 slots=1\rworker-3 slots=1 Lastly, we want to define our configuration file for DeepSpeed in deepspeed_config.json\n{ \u0026#34;train_batch_size\u0026#34;: 3, \u0026#34;train_micro_batch_size_per_gpu\u0026#34;: 1, \u0026#34;gradient_accumulation_steps\u0026#34;: 1, \u0026#34;fp16\u0026#34;: { \u0026#34;enabled\u0026#34;: true }, \u0026#34;zero_optimization\u0026#34;: { \u0026#34;stage\u0026#34;: 3 } } Inference First, define our package requirements in requirements.txt\ntorch\rtransformers\rdeepspeed In inference.py we use a small sentiment model and pass in a few statements to test with.\nimport os import torch import deepspeed from transformers import AutoModelForSequenceClassification, AutoTokenizer def main(): deepspeed.init_distributed() rank = int(os.environ.get(\u0026#34;RANK\u0026#34;, 0)) world_size = int(os.environ.get(\u0026#34;WORLD_SIZE\u0026#34;, 1)) model_name = \u0026#34;distilbert-base-uncased-finetuned-sst-2-english\u0026#34; tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForSequenceClassification.from_pretrained(model_name) ds_config = \u0026#34;deepspeed_config.json\u0026#34; model_engine, _, _, _ = deepspeed.initialize( model=model, config=ds_config, model_parameters=None ) statements = [ \u0026#34;I absolutely love this movie, it\u0026#39;s a masterpiece!\u0026#34;, \u0026#34;The service at this restaurant was terrible and the food was bland.\u0026#34;, \u0026#34;I\u0026#39;m feeling quite neutral about the whole situation.\u0026#34;, \u0026#34;This new gadget is amazing, it has exceeded all my expectations!\u0026#34;, \u0026#34;I\u0026#39;m really disappointed with the outcome of the game.\u0026#34; ] labels = [\u0026#34;Negative\u0026#34;, \u0026#34;Positive\u0026#34;] for input_text in statements: inputs = tokenizer(input_text, return_tensors=\u0026#34;pt\u0026#34;) inputs = {key: value.to(model_engine.device) for key, value in inputs.items()} with torch.no_grad(): outputs = model_engine(**inputs) logits = outputs.logits predictions = torch.argmax(logits, dim=-1) if rank == 0: print(f\u0026#34;Input: {input_text}\u0026#34;) print(f\u0026#34;Prediction: {labels[predictions.item()]}\u0026#34;) else: print(f\u0026#34;Worker {rank}: Inference completed for input: {input_text}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() To Test, navigate to your project folder and run:\ndocker compose build docker compose up After all of your workers initialize and the model is downloaded you should see each worker process our input statements and we expect Worker-1 to deliver the result for each.\n"},{"section":"Blog","slug":"/blog/aws-cdk-localstack/","title":"How to Set Up a Serverless Home Lab with AWS CDK, Lambda, and LocalStack","description":"Learn how to set up a local serverless environment using AWS CDK, LocalStack, and a simple Python Lambda function. Ideal for developers looking to test AWS services locally.","date":"September 26, 2024","image":null,"imageSM":null,"searchKeyword":"","categories":"Cloud, devops","tags":"AWS, LocalStack, AWS Lambda, Docker, AWS CDK - Serverless","content":"Are you looking to develop and your cloud application locally without incurring AWS costs? In this tutorial, we\u0026rsquo;ll guide you through setting up a local serverless environment using AWS CDK, LocalStack, and a simple Python Lambda function. We\u0026rsquo;ll leverage the PythonFunction construct from the aws-cdk.aws-lambda-python-alpha module to streamline the process.\nWhat is LocalStack? LocalStack is a fully functional local AWS cloud stack that allows you to develop and test your cloud applications offline. It emulates a vast majority of AWS services, enabling you to test your applications without connecting to the real AWS cloud. This is incredibly useful for development and testing purposes, as it reduces costs and increases development speed.\nPrerequisites Before we begin, ensure you have the following installed on your machine:\nPython 3.7+ Node.js npm AWS CLI AWS CDK Docker Setting Up LocalStack Install LocalStack using pip and then start it:\npip install localstack localstack start -d This will spin up LocalStack in the background, emulating AWS services locally.\nInstalling awslocal and cdklocal To interact with LocalStack using AWS CLI commands and AWS CDK, we\u0026rsquo;ll install awslocal and cdklocal. awslocal is a thin wrapper that runs AWS CLI commands against LocalStack.\npip install awscli-local npm install -g aws-cdk-local Creating a Lambda Function Let\u0026rsquo;s create a simple Lambda function that echoes back a message. We\u0026rsquo;ll also set up an API Gateway to invoke this Lambda function.\nCreate a new directory for your project and navigate into it:\nmkdir MyProject cd MyProject Initialize a New AWS CDK Project Initialize a new AWS CDK project in Python:\ncdklocal init app --language python This command sets up a basic CDK project structure with the necessary files.\nCreate the Lambda Function Directory Create a directory for your Lambda function:\nmkdir lambda Write the Lambda Function Inside the lambda directory, create a file named echo.py with the following content:\ndef handler(event, context): message = event.get(\u0026#39;message\u0026#39;, \u0026#39;Hello from Lambda!\u0026#39;) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: message } This simple function returns the message provided in the event, or a default message if none is provided.\nDefining the Infrastructure with AWS CDK Now, let\u0026rsquo;s define our infrastructure using AWS CDK.\nUpdate the CDK Stack First, add the aws-cdk.aws-lambda-python-alpha module to your project\u0026rsquo;s requirements.txt file:\naws-cdk.aws-lambda-python-alpha Install the required Python packages:\npip install -r requirements.txt Creating the CDK Stack Open MyProject.py and replace its content with the following code:\nfrom aws_cdk import ( Stack, aws_lambda as _lambda, aws_lambda_python_alpha as lambda_python, aws_apigateway as apigateway, ) from constructs import Construct class MyProjectStack(Stack): def __init__(self, scope: Construct, id: str, **kwargs) -\u0026gt; None: super().__init__(scope, id, **kwargs) echo_lambda = lambda_python.PythonFunction( self, \u0026#39;EchoFunction\u0026#39;, entry=\u0026#39;lambda\u0026#39;, index=\u0026#39;echo.py\u0026#39;, handler=\u0026#39;handler\u0026#39;, runtime=_lambda.Runtime.PYTHON_3_8, ) # Define the API Gateway REST API api = apigateway.LambdaRestApi( self, \u0026#39;EchoApi\u0026#39;, handler=echo_lambda, proxy=True ) Before deploying, bootstrap your CDK environment in LocalStack:\ncdklocal bootstrap Deploying to LocalStack Using CDK Now, let\u0026rsquo;s deploy our stack to LocalStack.\nInstall CDK Dependencies pip install -r requirements.txt Synthesize the CloudFormation Template cdklocal synth This command synthesizes your CDK app into a CloudFormation template, which is then used for deployment.\nDeploy the Stack cdklocal deploy The CDK will summarize the changes to your stack and you can press y to confirm the changes.\nOnce deployed, you\u0026rsquo;ll receive an endpoint URL for your API Gateway. You can test the Lambda function by sending a request to this URL.\nAfter deployment, note the API endpoint URL from the output. It should look something like:\nhttps://\u0026lt;api-id\u0026gt;.execute-api.localhost.localstack.cloud:4566/prod/ Test the API Use curl to send a POST request to the API endpoint:\ncurl -X POST \u0026#39;http://localhost:4566/restapis/\u0026lt;api-id\u0026gt;/prod/_user_request_\u0026#39; -d \u0026#39;{\u0026#34;message\u0026#34;: \u0026#34;Hello, LocalStack!\u0026#34;}\u0026#39; Replace \u0026lt;api-id\u0026gt; with the actual API ID from your deployment output.\nExpected Response You should receive a response similar to:\n{ \u0026#34;statusCode\u0026#34;: 200, \u0026#34;body\u0026#34;: \u0026#34;Hello, LocalStack!\u0026#34; } Thoughts You\u0026rsquo;ve successfully set up a local serverless environment using AWS CDK and LocalStack. Using LocalStack and AWS CDK together, you can run most AWS services through LocalStack so that you can build and test your entire system locally. We\u0026rsquo;ve set up a simple lambda and an api gateway, but this project could be extended to include local versions of services like DynamoDB, Cognito and SQS with only a bit more code.\n"}]